# 딥러닝의 깊이 있는 이해를 위한 머신 러닝
| **@KMOOC**

| **중앙대학교 최종원 교수님**

| **https://lms.kmooc.kr/course/view.php?id=11397**
## *W1* : 머신 러닝의 소개 
### *머신 러닝의 핵심은 방대한 데이터 내에서 특징을 자동으로 찾아내는 것!*
> ### 방대한 데이터의 표현 방법
> <**2차원의 행렬**>
> | *feature 1* | *feature 2* | *feature 3* | *feature 4* | *feature 5* |
> |---|---|---|---|---|
> | value | value | value | value | value |
> | value | value | value | value | value |
> | value | value | value | value | value |
> | ... | ... | ... | ... | ... |
>
> **Record** : (행렬에서의 한 행 = **row**)
> | *feature 1* | *feature 2* | *feature 3* | *feature 4* | *feature 5* |
> |---|---|---|---|---|
> | value | value | value | value | value |
>
> **Field** : *feature* 
>
> **Domain** : 중복되지 않는 *feature* value 의 집합

> ### 방대한 데이터의 조작이 필요한 이유
> 일반적으로 *feature* value는 숫자인 것과 숫자가 아닌 것으로 구분, 머신 러닝은 컴퓨터가 데이터 내에서 특징을 찾아가는 방식으로 진행되는데, 컴퓨터는 숫자가 아닌 *feature* value 를 인식하지 못한다. 이것은 큰 문제가 될 수 있고, 또한 *feature* value 가 숫자라고 하더라도, 그 크기가 너무 작거나 너무 크면(*outlier*) 이 역시 문제가 될 수 있다. 마지막으로 있어야 할 *feature* value 가 없는 경우에도 문제가 되기에 방대한 데이터의 조작은 필수적이다.
>
> **조작 없이 사용하면 문제가 될 수 있는 대표적인 데이터의 형태** : 문장 데이터, 음성 데이터, 이미지 데이터, 그래프 데이터, 이상치(*outlier*), *NULL*(*NaN*, *None* 등)

> ### 머신 러닝에 필수적인 수학 개념
> **미적분학**, **선형대수학**, **통계학**, **이산수학**

> ### 머신 러닝의 대분류
> **회귀**, **분류**
> 
## *W2* : Decision Tree와 지도 학습
> 일반적인 트리 형태의 그래프 상에서 *feature* value 를 기준으로 특정 조건에 부합하는 노드만을 거쳐 최종 결과에 도달하는 가장 이해하기 쉬운 머신 러닝 기법이다.
> 
> ![image](https://github.com/CharmStrange/Playground/assets/105769152/c467ba40-3eab-4a1c-b91e-591686c23f6b)
>
> 이 그림에서의 **Decision Tree**는 가장 쉬운 `예-아니오` 이진 분류 형태이고, 최종 결과(목적지)를 **Label** 이라고 부른다.
>
> **Label** 에 대해서 집중적으로 예측을 진행하는 것이 머신 러닝의 **지도 학습** 기법이고, 이 기법은 사용자가 모델에 *fature* value 와 그에 대한 **Label** 을 제공하는 방식으로 진행된다. 그럼 모델은 주어진 **Label** 과 그에 대응하는 *faeture* value 들의 특징을 자동으로 찾아내 학습을 하는 것이다.
> 
> | *feature 1* | *feature 2* | *feature 3* | *feature 4* | **Label** |
> |---|---|---|---|---|
> | value | value | value | value | 1 |
> 
> 일반적으로 하나의 *feature* value 와 **Label** 이 포함된 **Record** 가 모델에 제공된다.
> 

## *W3* : 지도 학습의 일반화 성능
> 보통 방대한 데이터가 주어졌을 때, **Decision Tree**는 수많은 **Record** 에 대해 Decision을 하는 Tree 진행 알고리즘을 수행한다. 이를 **Decision Tree** *split* 이라 부르고, 이것이 어떻게 구성되느냐에 따라 **Decision Tree**의 성능이 결정된다. 
>
> 여기서 성능이라는 것은, 기존의 방대한 데이터에 포함되지 않은, 새로운 *feature* value 의 **Record** 가 모델에 주어졌을 때, 지도 학습을 바탕으로 습득한 데이터의 패턴에 따라 **Label** 을 얼마나 잘 예측하느냐의 척도이다.
>
> ### 일반화 개요
> 모델의 성능을 향상시키려면, 즉 새로운 데이터에도 강한 예측력을 보여주는 모델을 만들기 위해선 데이터 학습 단계에서 모델이 데이터의 특징을 일반화할 수 있게 잘 조정해야 한다. 데이터의 특징을 일반화하지 못하고 패턴을 학습한다면 **Overfitting** 또는 **Underfitting** 이라는 문제가 발생한다.
>
> > #### Overfitting
> 학습에 사용된 데이터에 한해서만 **Label** 예측이 정확한, 데이터의 모든 특징을 너무 과도하게 학습하여 발생하는 문제이다. 또한 과도한 학습이 이루어지지 않았더라도 데이터의 양이 너무 적으면 발생하기도 한다.
>
> > #### Underfitting
> 학습에 사용된 데이터의 **Label** 도 제대로 예측하지 못하는, 데이터의 특징을 너무 대충, 적게 학습하여 발생하는 문제이다.
>
> > #### Overfitting & Underfitting 검증 방법
> ![image](https://github.com/CharmStrange/Playground/assets/105769152/7e2c893b-881b-42f4-afea-d5620a6e5116)
>
> **Train Error** & **Test Error** 라는 모델의 정확성 척도를 각각 계산하고, 이 둘의 차이를 비교해, 그 차이가 작은 지점이 일반화가 잘 된 현재로서 최적 상태인 모델이다. 만약 이 차이가 크게 벌어지면 **Overfitting** 이 발생한 것으로 판단한다.
>
> > #### 일반화를 잘 시키는 방법
> - 방대한 양의 데이터를 준비한다.
> - 지도 학습 모델을 너무 복잡하게 구성하지 않는다.
> - 모델 학습 시 데이터의 독립성을 가정하여 데이터를 여러 블록으로 쪼개 따로 학습한다.
> - 모델 학습 시 데이터의 독립성을 가정하여 무작위로 복원추출하는 과정을 사용한다.

## *W4* : 머신 러닝 기법 : 분류 (1)
> ### 확률적 구분기의 정의
> *feature* value : $x$ 가 주어졌을 때, $y$ 가 어떤 결과를 낼 지를 확률적으로 결정하는 기계이다.
>
>  $P(y=?|x)$ 의 조건부 확률을 기반으로 동작하게 된다.
>
> > **Bayes Rule** 
> >
> > $P(A|B) = \frac{P(B|A)P(A)}{P(B)} $
> >
> > $P(A|B)$ : *A given B* 는 $B$라는 사건이 주어졌을 때의 사건 $A$가 발생할 확률을 나타내는데, **Bayes Rule**에 따르면 이것을 계산하기 위해서 $P(B|A)$, 즉 사건 $A$가 주어졌을 때의 사건 $B$가 발생할 확률이 필요하게 된다. 
> >
> > 일반적으로 분류 문제에서는 **Label** 의 형태가 단순하다. 하지만 **Label** 을 예측하기 위해 사용되는 *feature* value 들은 여러 형태를 가져 복잡하다. - 여기서 목표로 하는 확률을 정의하면 다음과 같이 표현이 가능하다 : $P(Label|fv)$
> >
> >머신 러닝의 분류 문제에서 **Bayes Rule** 없이, 목표로 하는 확률 $P(Label|fv)$를 구하려고 한다면, 복잡성이 높은 모든 *feature* value 들을 전부 거치고 난 뒤 그 중에서 목표로 하는 **Label** 을 찾아야만 한다. 그런데 **Bayes Rule** 을 도입한다면?
> >
> > $P(Label|fv) = \frac{P(fv|Label)P(Label)}{P(fv)} $ , $P(fv|Label)$만 있으면 된다. 이는 곧 지도 학습을 통한 목표로 하는 **Label** 을 먼저 정해준 뒤, 데이터에서 그것에 해당하는 것만 골라 분류 모델의 학습을 진행하는 것과 같아진다. 
>
> > **Naive Bayes Algorithm** & *Joint Probability*
> >
> > **Bayes Rule** 을 도입한 분류 모델에서는 데이터의 *feature* value 들이 각각 독립이라고 가정한다. 그렇다면 $P(Label|fv)$를 *Joint Probability*로 풀어 표현할 수 있게 된다 :
> >
> > > *Joint Probability*
> > > 
> > > 두 개의 확률적 사건(변수)이 동시에 발생할 확률을 나타낸 것.
> > > 
> > > $P(A, B) = P(B, A) = P(A \cap B)$  
> >
> > $P(Label|fv) = P(fv_0|Label)P(fv_1|Label)P(fv_2|Label)P(fv_3|Label)...$
> >
> > 결과적으로 하나의 **Label** 에 대해서 그에 대응하는 각각의 *feature* value 의 확률을 모두 곱한 결과가 목표로 하는 **Label** 의 확률인 것이다.

## *W5* : 머신 러닝 기법 : 분류 (2)
> ### Non-parametric 구분기의 정의 
> 모델의 파라미터 개수가 고정되어 있거나 데이터의 개수에 비례하는 분류 모델이다. 이러한 특징 때문에 보통 데이터의 양이 증가할수록 최종 분류 결과의 정확도도 증가하는 경향을 보인다. 
> 
> ### 대표적인 Non-parametric 구분기
> - Decision Tree
> - K - Nearest Neighbor Classifier : 비슷한 패턴을 가지는 데이터끼리 군집화하여 **Label** 을 결정하는 기법
> - KNN Hierarchical Tree 
>
> 그러나 이 때문에 발생하는 문제도 있다. *Curse of Dimensionality* : 데이터의 양이 많을수록 좋긴 하지만 동시에 복잡도가 커진다. 결국, 무작정 데이터의 양이 많다고 좋은 분류 모델을 만들 수 있는 것이 아니라, 사용되는 데이터의 복잡도가 너무 크거나 작은, 또는 데이터의 분포 문제가 없어야 성능이 좋은 분류 모델을 만들 수 있다는 것이다.
>
> $+$ ***Decision Theory*** : 분류기가 제대로 된 **Label** 예측을 수행했는지에 대한 결과에, **cost** 를 부여해 그것을 평가하고 최적의 **Label** 을 찾으려면 어떻게 하면 되는지를 판단하는 일종의 최적화 이론이다.
>

## W6 : 앙상블 모델
> ***Ensemble*** : 전체적인 어울림이나 통일. ‘조화’로 순화한다는 의미의 프랑스어
> 
> ![image](https://github.com/CharmStrange/Playground/assets/105769152/fa69b58b-fc7a-465c-add9-9c7b3954d938)

> 
> 이름에서도 알 수 있듯, 여러 분류 모델을 섞어 그 결과에 있어 최선의 성능을 발휘할 수 있게 하는 새로운 모델이다. `Meta-Classfier` 라고도 불리며, 직관적으로 이해가 가능한 동시에 결과에 대한 성능이 단적 분류 모델에 비해 뛰어난 편이라 머신 러닝 세계에서 매우 자주 활용된다.
>
> ![image](https://github.com/CharmStrange/Playground/assets/105769152/060bcf83-1129-47eb-a8b7-ef339c184e38)

>
> ### 대표적인 앙상블 모델(기법) 
> - **Averaging**
> - **Bagging - Bootstrapping**
> - **Boosting - Cascading**
>
> >  **Averaging** 예시
> > | 모델 | 정확도 1 | 정확도 2 | 정확도 3 | 정확도 4 |
> > |---|---|---|---|---|
> > | 모델 A | 0.8 | 0.8 | 0.8 | 0.2 | 
> > | 모델 B | 0.8 | 0.8 | 0.2 | 0.2 | 
> > | 모델 C | 0.8 | 0.2 | 0.2 | 0.2 | 
> > |---|---|---|---|---|
> > | 계(정확도) | 0.512 | 0.384 | 0.096 | 0.008 |
> >
> > 단적 분류 모델이 옳게 **Label** 을 예측할 확률은 80%, **Averaging** 을 통한 앙상블 모델이 **Label** 을 옳게 예측할 확률은 89.6%(0.512 + 0.384)로, 앙상블 모델이 기존 단적 모델보다 성능이 약 10% 향상된 것을 볼 수 있다.
>
> > **Bagging - Bootstrapping**
> > 
> > **Bootstrapping** : 독립성을 가지는 기존의 데이터셋으로, 독립성을 여전히 가지는 새로운 데이터셋 집합을 만들어 서로 다른 분류 모델이 새로운 데이터셋 각각을 학습하는 방법이다. 
> > 
> > **Bootstrapping** 을 기반으로 여러 분류 모델이 내놓은 결과를 종합하여 최종 결정을 하는 기법이 **Bagging** 이다.
> >
> > Decision Tree 에 **Bagging** 을 적용하여 **Random Tree** 모델을 만들 수 있고, 이들이 모여 **Random Forest** 가 된다.
> 
> > **Boosting - Cascading**
> >
> > **Boosting** 은 성능 향상에 목적을 두었다기보다도 속도 향상에 그것을 둔 새로운 기법이다. 앙상블 모델을 구성하는 단계에서, 각 모델의 순서를 정하는데, 이 순서는 각 분류 모델의 복잡도가 올라갈수록 후순위로 지정된다. 
> >
> > | 매우 간단 | 간단 | 무난 | 보통 | 조금 복잡 | 꽤 복잡 | 많이 복잡 | 매우 복잡 |
> > |---|---|---|---|---|---|---|---|
> > | 모델 1 | 모델 2 | 모델 3 | 모델 4 | 모델 5 | 모델 6 | 모델 7 | 모델 8
> > 
> > ` 모델 1 > 모델 2 > 모델 3 > 모델 4 > 모델 5 > 모델 6 > 모델 7 > 모델 8`
> > 
> > 각각의 모델에서 나온 결과를 바탕으로, 현재 이 정도면 충분한지, 모델을 더 거쳐야 만족할 만한 결과가 나오는지를 판단하는 기법이다. 
> >
> > 이 기법은 초반부터 모델의 학습을 복잡하게 진행하지 않고 최종 결정을 섬세하게 판단할 수 있다는 장점이 있다.
